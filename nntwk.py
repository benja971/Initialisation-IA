# -*- coding: utf-8 -*-
"""TP-projet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KeQEtCtocjCX40rV_8ychIRsg4dNhl_x
"""

import os
from tabnanny import verbose
import tensorflow as tf
import matplotlib.pyplot as plt
import pandas as pd
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from time import time

datas = pd.read_csv("./whites.csv", sep=";")
datas.info()

datas.quality -= 1

y = datas.quality
x = datas.drop(['quality'], axis=1)

x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.3, random_state=42)

scaler = preprocessing.StandardScaler().fit(x_train)

# Et on normalise les 2 bases
x_train_norm = scaler.transform(x_train)
x_test_norm = scaler.transform(x_test)


# sur apprentissage
# model = tf.keras.models.Sequential([
#     tf.keras.Input(shape=(11), name='input'),
#     tf.keras.layers.Dense(50, activation='relu'),
#     tf.keras.layers.Dense(50, activation='relu'),
#     tf.keras.layers.Dense(10)
# ])
for i in [4, 8, 12]:
    # pas mal
    model = tf.keras.models.Sequential([
        tf.keras.Input(shape=(11), name='input'),
        tf.keras.layers.Dense(i, activation='relu'),
        tf.keras.layers.Dense(i, activation='relu'),
        tf.keras.layers.Dense(10)
    ])

    # loss func for classif
    # loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

    # loss func for regression
    # loss_fn = tf.keras.losses.MeanAbsoluteError()
    loss_fn = tf.keras.losses.MeanSquaredError()

    model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])

    # model.summary()

    BATCH_SIZE = 100

    # La base d'apprentissage
    dataset = tf.data.Dataset.from_tensor_slices((x_train_norm, y_train))
    dataset = dataset.shuffle(x_train_norm.shape[0])
    dataset = dataset.batch(BATCH_SIZE)

    train_dataset = dataset

    # La base de validation
    dataset = tf.data.Dataset.from_tensor_slices((x_test_norm, y_test))
    dataset = dataset.shuffle(x_test_norm.shape[0])
    dataset = dataset.batch(BATCH_SIZE)

    test_dataset = dataset

    start = time()
    history = model.fit(
        train_dataset, validation_data=test_dataset, epochs=200, verbose=0)
    os.system("cls")

    print("Temps d'apprentissage : {:.2f}s".format(time() - start))

    # summarize history for accuracy
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    # plt.show()
    plt.savefig('./images/11-'+str(i)+'-'+str(i) +
                '/model accuracy (Mean squared error).png')

    plt.clf()

    # summarize history for accuracy
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Loss function (Mean squared error)')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    # plt.show()
    plt.savefig('./images/11-'+str(i)+'-'+str(i) +
                '/Loss function (Mean squared error).png')
    plt.clf()
