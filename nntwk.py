# -*- coding: utf-8 -*-
"""TP-projet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KeQEtCtocjCX40rV_8ychIRsg4dNhl_x
"""

import os
import tensorflow as tf
import matplotlib.pyplot as plt
import pandas as pd
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from time import time

"fixed acidity"
"volatile acidity"
"citric acid"
"residual sugar"
"chlorides"
"free sulfur dioxide"
"total sulfur dioxide"
"density"
"pH"
"sulphates"
"alcohol"
"quality"

datas = pd.read_csv("./whites.csv", sep=";")
datas.info()

datas.quality -= 1

y = datas.quality
# , 'alcohol', 'density', 'free sulfur dioxide'
x = datas.drop(['quality', 'chlorides', 'citric acid',
               'pH', 'sulphates'], axis=1)


x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.3, random_state=42)

scaler = preprocessing.StandardScaler().fit(x_train)

# Et on normalise les 2 bases
x_train_norm = scaler.transform(x_train)
x_test_norm = scaler.transform(x_test)


# sur apprentissage
# model = tf.keras.models.Sequential([
#     tf.keras.Input(shape=(11), name='input'),
#     tf.keras.layers.Dense(50, activation='relu'),
#     tf.keras.layers.Dense(50, activation='relu'),
#     tf.keras.layers.Dense(10)
# ])

for i in [4, 8, 12, 16, 20]:
    # pas mal
    model = tf.keras.models.Sequential([
        tf.keras.Input(shape=(7), name='input'),
        tf.keras.layers.Dense(i, activation='relu'),
        tf.keras.layers.Dense(i, activation='relu'),
        tf.keras.layers.Dense(i, activation='relu'),
        tf.keras.layers.Dense(1)
    ])

    # loss func for classif
    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

    # loss func for regression
    loss_fn = tf.keras.losses.MeanAbsoluteError()
    loss_fn = tf.keras.losses.MeanSquaredError()

    model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])

    # model.summary()

    BATCH_SIZE = 100

    # La base d'apprentissage
    dataset = tf.data.Dataset.from_tensor_slices((x_train_norm, y_train))
    dataset = dataset.shuffle(x_train_norm.shape[0])
    dataset = dataset.batch(BATCH_SIZE)

    train_dataset = dataset

    # La base de validation
    dataset = tf.data.Dataset.from_tensor_slices((x_test_norm, y_test))
    dataset = dataset.shuffle(x_test_norm.shape[0])
    dataset = dataset.batch(BATCH_SIZE)

    test_dataset = dataset

    start = time()
    history = model.fit(
        train_dataset, validation_data=test_dataset, epochs=200, verbose=0)
    os.system("cls")

    plt.clf()

    print("Temps d'apprentissage : {:.2f}s".format(time() - start))
    # summarize history for accuracy
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    # plt.show()
    # check if folder exists
    if not os.path.exists('./images/8-'+str(i)+'-'+str(i) + '-'+str(i)):
        os.makedirs('./images/8-'+str(i)+'-'+str(i) + '-'+str(i))

    plt.savefig('./images/8-'+str(i)+'-'+str(i) + '-'+str(i) +
                '/model accuracy(Mean Squared Error).png')

    plt.clf()

    # summarize history for accuracy
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Loss function (Mean Squared Error)')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    # plt.show()
    plt.savefig('./images/8-'+str(i)+'-'+str(i) + '-'+str(i) +
                '/Loss function(Mean Squared Error).png')
    plt.clf()
