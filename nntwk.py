# -*- coding: utf-8 -*-
"""TP-projet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KeQEtCtocjCX40rV_8ychIRsg4dNhl_x
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import pandas as pd
from sklearn import preprocessing
from sklearn.model_selection import train_test_split

datas = pd.read_csv("./whites.csv", sep=";")
datas.info()

datas.quality -= 1

y = datas.quality
x = datas.drop(['quality'], axis=1)

x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.3, random_state=42)

scaler = preprocessing.StandardScaler().fit(x_train)

# Et on normalise les 2 bases
x_train_norm = scaler.transform(x_train)
x_test_norm = scaler.transform(x_test)

print(x_train_norm)
print(x_test_norm)

# sur apprentissage
# model = tf.keras.models.Sequential([
#     tf.keras.Input(shape=(11), name='input'),
#     tf.keras.layers.Dense(50, activation='relu'),
#     tf.keras.layers.Dense(50, activation='relu'),
#     tf.keras.layers.Dense(10)
# ])

# pas mal
model = tf.keras.models.Sequential([
    tf.keras.Input(shape=(11), name='input'),
    tf.keras.layers.Dense(5, activation='relu'),
    tf.keras.layers.Dense(5, activation='relu'),
    tf.keras.layers.Dense(10)
])

# loss func for classif
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# loss func for regression
# loss_fn = tf.keras.losses.MeanSquaredError()

model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])

model.summary()

BATCH_SIZE = 100

# La base d'apprentissage
dataset = tf.data.Dataset.from_tensor_slices((x_train_norm, y_train))
dataset = dataset.shuffle(x_train_norm.shape[0])
dataset = dataset.batch(BATCH_SIZE)

train_dataset = dataset

# La base de validation
dataset = tf.data.Dataset.from_tensor_slices((x_test_norm, y_test))
dataset = dataset.shuffle(x_test_norm.shape[0])
dataset = dataset.batch(BATCH_SIZE)

test_dataset = dataset

history = model.fit(train_dataset, validation_data=test_dataset, epochs=200)

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
plt.save("model_accuracy_classif")

# summarize history for accuracy
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Fonction de perte (entropie croisee)')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
plt.save("pert_func_classif")